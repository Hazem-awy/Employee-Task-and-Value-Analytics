{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## coverthe spreadsheet into csv fiels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the file path\n",
    "file_path = r'C:\\Users\\Hazem Hamdy\\Desktop\\mobadra\\voda.xlsx'\n",
    "\n",
    "# Load the Excel file\n",
    "excel_data = pd.ExcelFile(file_path)\n",
    "\n",
    "# Get the names of the sheets in the Excel file\n",
    "sheet_names = excel_data.sheet_names\n",
    "\n",
    "# Convert each sheet to a CSV file\n",
    "for sheet_name in sheet_names:\n",
    "    # Read the sheet into a DataFrame\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    \n",
    "    # Define the CSV file path\n",
    "    csv_file_path = f'C:\\\\Users\\\\Hazem Hamdy\\\\Desktop\\\\mobadra\\\\csc_voda\\\\{sheet_name}.csv'\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(csv_file_path, index=False)\n",
    "    print(f\"Saved {sheet_name} to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat the data and add the locatoin column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory containing the CSV files\n",
    "directory = r'C:\\Users\\Hazem Hamdy\\Desktop\\mobadra\\csc_voda'\n",
    "\n",
    "# Use glob to find all CSV files in the directory\n",
    "file_paths = glob.glob(os.path.join(directory, '*.csv'))\n",
    "\n",
    "# Create an empty DataFrame to hold the combined data\n",
    "combined_df1 = pd.DataFrame()\n",
    "\n",
    "# Process each file\n",
    "for file_path in file_paths:\n",
    "    # Extract the location name from the file name\n",
    "    location = os.path.basename(file_path).split('.')[0]\n",
    "    \n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Ensure 'DIM' and 'Task' columns exist\n",
    "    if 'DIM' not in df.columns or 'Task' not in df.columns:\n",
    "        continue\n",
    "    \n",
    "    # Add 'Location' column with the extracted location name\n",
    "    df['Location'] = location\n",
    "    \n",
    "    # Reorder columns to have 'Location' at the beginning\n",
    "    df = df[['Location', 'DIM', 'Task', 'APPROVAL'] + [col for col in df.columns if col not in ['Location', 'DIM', 'Task', 'APPROVAL']]]\n",
    "    \n",
    "    # Append to combined DataFrame\n",
    "    combined_df1 = pd.concat([combined_df1, df], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "# combined_df1.to_csv(r'C:\\Users\\Hazem Hamdy\\Desktop\\mobadra\\csc_voda\\combined_data_with_location.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert all null in dim , taks approvel coulmn into N\\A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values in 'DIM' and 'Task' columns with 'N/A'\n",
    "combined_df1[['DIM', 'Task', 'APPROVAL']] = combined_df1[['DIM', 'Task' , 'APPROVAL']].fillna('N/A')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## melt the data frame and remove the date rows from emp code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the date coulmn\n",
    "combined_df = combined_df1.melt(id_vars=['Location','DIM', 'Task', 'APPROVAL'], var_name='Date', value_name='Value')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "employee_codes = combined_df['DIM'].dropna().unique()\n",
    "values_to_remove = ['Contracted Hours', 'Allocated Hours', 'Remaining', 'Technical',\"Planning\"]\n",
    "df_employee_codes = pd.DataFrame(employee_codes, columns=['EmployeeCode'])\n",
    "df_employee_codes_filtered = df_employee_codes[~df_employee_codes['EmployeeCode'].isin(values_to_remove)]\n",
    "\n",
    "# Go through each row to update 'Date' column\n",
    "for index, row in combined_df.iterrows():\n",
    "    if row['DIM'] in df_employee_codes_filtered['EmployeeCode'].values:\n",
    "        if pd.notna(row['Date']):\n",
    "            combined_df.at[index, 'Date'] = ''\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "# combined_df.to_csv(r'C:\\Users\\Hazem Hamdy\\Desktop\\mobadra\\csc_voda\\combined_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the data of each employee and put it in another column,that Data assigned to it's emp_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize temp to None\n",
    "temp = None\n",
    "\n",
    "# Create new columns 'subdim' and 'dim1'\n",
    "combined_df['Sub-Dim'] = ''\n",
    "combined_df['Dim'] = ''\n",
    "\n",
    "# Iterate through the rows of combined_df\n",
    "for index, row in combined_df.iterrows():\n",
    "    if row['DIM'] in df_employee_codes_filtered['EmployeeCode'].values:\n",
    "        # If DIM value is an employee code, set temp\n",
    "        temp = row['DIM']\n",
    "    else:\n",
    "        # If DIM value is not an employee code, set 'subdim' and 'dim1'\n",
    "        combined_df.at[index, 'Sub-Dim'] = row['DIM']\n",
    "        combined_df.at[index, 'Dim'] = temp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the emp_code from the column dim first and after  drop the old Dim coulms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if each value in 'DIM' exists in 'df_employee_codes_filtered' and remove those rows\n",
    "combined_df_sin_emp = combined_df[~combined_df['DIM'].isin(df_employee_codes_filtered['EmployeeCode'])]\n",
    "combined_df_sin_emp = combined_df_sin_emp.drop(columns=['DIM'])[['Location', 'Dim', 'Sub-Dim', 'Task', 'APPROVAL', 'Date', 'Value']]\n",
    "combined_df_final = combined_df_sin_emp.dropna(subset=['Sub-Dim'])\n",
    "# combined_df_final.to_csv(r'C:\\Users\\Hazem Hamdy\\Desktop\\mobadra\\csc_voda\\combined_data_final.csv', index=False)\n",
    "# combined_df_final.to_csv(r'C:\\Users\\Hazem Hamdy\\Desktop\\mobadra\\csc_voda\\Emp_data_State_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start with the Emp DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_dep = pd.read_csv('C:\\\\Users\\\\Hazem Hamdy\\\\Desktop\\\\mobadra\\\\voda_employee\\\\Employee Dept.csv')\n",
    "employee_cost = pd.read_csv('C:\\\\Users\\\\Hazem Hamdy\\\\Desktop\\\\mobadra\\\\voda_employee\\\\Employee HR Cost.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Employee Dept & Employee HR Cost files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_cost_dep = pd.merge(employee_dep,employee_cost, on='Employee',how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_cost_dep.to_csv(r'C:\\\\Users\\\\Hazem Hamdy\\\\Desktop\\\\mobadra\\\\voda_employee\\\\employee_cost_dep.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the two prevous dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_agg = pd.merge(combined_df_final,employee_cost_dep, left_on='Dim' , right_on='Employee',how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_agg = emp_agg.drop(columns=['Employee'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_agg.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_agg_filterd= emp_agg.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_agg_filterd.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_agg_filterd.to_csv(r'C:\\\\Users\\\\Hazem Hamdy\\\\Desktop\\\\mobadra\\\\voda_employee\\\\employee_agg.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analsis&vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data into a DataFrame (skip this step if you already have the DataFrame)\n",
    "df = pd.read_csv('C:\\\\Users\\\\Hazem Hamdy\\\\Desktop\\\\mobadra\\\\voda_employee\\\\employee_agg_final_final.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Task Distribution by Employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of tasks per employee\n",
    "task_counts = df['Dim'].value_counts()\n",
    "# Get the top 10 employees by task count\n",
    "top_10_task_counts = task_counts.head(10)\n",
    "\n",
    "# Set the size of the plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot the data for the top 10 employees\n",
    "sns.barplot(x=top_10_task_counts.index, y=top_10_task_counts.values, palette=\"viridis\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Employee')\n",
    "plt.ylabel('Number of Tasks')\n",
    "plt.title('Task Distribution for Top 10 Employees')\n",
    "plt.xticks(rotation=45)  # Rotate x labels for better readability\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Location-Based Task Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the data for task counts per location and approval status\n",
    "location_task_analysis = df.groupby(['Location', 'APPROVAL']).size().unstack()\n",
    "\n",
    "# Plot the stacked bar chart\n",
    "location_task_analysis.plot(kind='bar', stacked=True, figsize=(12, 8), color=['#4CAF50', '#FFC107'])\n",
    "plt.title('Location-Based Task Analysis')\n",
    "plt.xlabel('Location')\n",
    "plt.ylabel('Number of Tasks')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Approval Status')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Approval partial Status Across Different Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data for tasks with 'Partial' approval\n",
    "partial_approval_data = df[df['APPROVAL'] == 'Partial']\n",
    "\n",
    "# Count the number of partial approval statuses per task\n",
    "partial_approval_status = partial_approval_data['Task'].value_counts().head(15)\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "partial_approval_status.plot(kind='bar', color='green')\n",
    "plt.title('Number of Partial Approval Tasks per Task')\n",
    "plt.xlabel('Task')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=70)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filter the data for the specific task 'TASK_T0397'\n",
    "task_data = partial_approval_data[partial_approval_data['Task'] == 'TASK_T0397']\n",
    "\n",
    "# 2. Count the number of times each employee has been assigned to this task\n",
    "employee_counts = task_data['Dim'].value_counts()\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "employee_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Number of Times Each Employee is Assigned to TASK_T0397')\n",
    "plt.xlabel('Employee Code')\n",
    "plt.ylabel('Number of Assignments')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()  # Adjust layout to fit labels\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Task partial Rate by Employee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data for tasks with 'Partial' approval\n",
    "partial_approval_data = df[df['APPROVAL'] == 'Partial']\n",
    "\n",
    "# Count the number of partial approval statuses per task\n",
    "partial_approval_status = partial_approval_data['Dim'].value_counts().head(15)\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "partial_approval_status.plot(kind='bar', color='purple')\n",
    "plt.title('Number of Partial Approval Tasks per Task')\n",
    "plt.xlabel('Task')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sub-Dimension vs. Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average cost per Sub-Dim\n",
    "avg_cost_per_subdim = df.groupby('Sub-Dim')['Value'].mean()\n",
    "\n",
    "# Plot the bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "avg_cost_per_subdim.plot(kind='bar', color='orange')\n",
    "plt.title('Average Value per Sub-Dimension')\n",
    "plt.xlabel('Sub-Dimension')\n",
    "plt.ylabel('Average Value')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pie char to each level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each unique value in LEVEL-01\n",
    "level_03_counts = df['LEVEL-03'].value_counts()\n",
    "\n",
    "# Plot pie chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(level_03_counts, labels=level_03_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('LEVEL-03 Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each unique value in LEVEL-01\n",
    "level_02_counts = df['LEVEL-02'].value_counts()\n",
    "\n",
    "# Plot pie chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(level_02_counts, labels=level_02_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('LEVEL-02 Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each unique value in LEVEL-01\n",
    "level_01_counts = df['LEVEL-01'].value_counts()\n",
    "\n",
    "# Plot pie chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(level_01_counts, labels=level_01_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('LEVEL-01 Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Monthly Value Trends "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct date format and parse the 'Date' column\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%d-%b')\n",
    "\n",
    "\n",
    "# Aggregating total value by month\n",
    "monthly_value = df.groupby(df['Date'].dt.to_period('M'))['Value'].sum()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "monthly_value.plot(kind='line', marker='o')\n",
    "plt.title('Monthly Value Distribution')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Value')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
